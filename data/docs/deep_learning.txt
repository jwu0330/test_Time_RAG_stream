第一篇　深度學習的理論基礎
第一章　人工智慧與深度學習的演進
1.1　人工智慧的起源：從邏輯到學習

人工智慧（Artificial Intelligence, AI）的歷史可追溯至 1950 年代。當時的研究者試圖讓電腦模仿人類思考與推理。艾倫·圖靈（Alan Turing）於 1950 年提出著名的「圖靈測試（Turing Test）」，成為評估機器是否具備「智能」的重要概念依據。

**第一波 AI 浪潮（1950–1970）**以「符號主義（Symbolic AI）」為主。這種方法強調邏輯推理與規則系統，認為人類知識可以透過「邏輯命題」與「推論規則」加以表達。例如：

「如果 X 是鳥，那麼 X 會飛」

「如果企鵝是鳥，那麼企鵝不會飛」

這類知識可以編碼成專家系統（Expert System），如 MYCIN 與 DENDRAL。
然而，符號主義 AI 面臨兩個根本困難：

知識獲取瓶頸（Knowledge Acquisition Bottleneck）：需要人工輸入大量規則。

缺乏學習能力：當環境或資料變化時，系統無法自我調整。

1.2　第二波浪潮：機器學習的興起

1980–2000 年代，隨著資料量與運算能力提升，人工智慧進入第二波浪潮——機器學習（Machine Learning, ML）。此階段的核心思想是：「讓機器從資料中自動學習規則」，而非由人類手動設計。

傳統機器學習演算法（如決策樹、SVM、隨機森林、KNN）多依賴特徵工程（Feature Engineering）。也就是說，資料科學家需根據任務特性（如影像的顏色、紋理、形狀等）人工設計輸入特徵。

這種方法在中小型資料集上效果良好，但隨著資料維度（Dimension）爆炸式增加——特別是影像、語音與文字等**非結構化資料（Unstructured Data）**出現後，傳統方法逐漸顯得捉襟見肘。

1.3　第三波浪潮：深度學習的誕生

**深度學習（Deep Learning, DL）**源自於對「人工神經網絡（Artificial Neural Network, ANN）」的再發掘。

雖然神經網絡的概念早在 1957 年由 Rosenblatt 提出的「感知機（Perceptron）」就已存在，但由於當時的計算能力不足、理論基礎不穩，加上 Minsky 與 Papert 在 1969 年證明「單層感知機無法處理非線性問題（如 XOR）」後，神經網絡研究陷入低潮。

直到 2006 年，Hinton、LeCun 與 Bengio 等人提出「深層神經網絡（Deep Neural Network）」與「逐層預訓練（Layer-wise Pretraining）」技術，配合 GPU 平行運算與大數據資料集（如 ImageNet），深度學習才重新崛起，並掀起了第三次 AI 革命。

1.4　從人工設計到自我表徵

深度學習最核心的特徵是自動特徵學習（Automatic Feature Learning）。
在傳統 ML 中，特徵工程是由人類專家主導；但在 DL 中，模型本身會自動學習哪些特徵最能代表輸入資料。

以影像為例：

第一層學習邊緣（edges）；

第二層學習紋理（textures）；

第三層學習形狀（shapes）；

更高層學習物體結構（objects）。

這種逐層抽象的特徵表示方式稱為分層表徵（Hierarchical Representation），是深度學習能夠突破傳統限制的關鍵。

1.5　人工智慧的三大典範

AI 的三大主要學習範式可分為：

類型	核心概念	代表演算法	應用場景
監督式學習（Supervised Learning）	給模型「輸入 + 正確答案」學習	CNN、RNN、Transformer	圖像分類、語音辨識
非監督式學習（Unsupervised Learning）	只給輸入，模型自找規律	Autoencoder、GAN、Clustering	特徵壓縮、生成模型
強化學習（Reinforcement Learning）	模型在環境中試錯，根據獎勵調整策略	Q-Learning、PPO、DQN	AlphaGo、機器人控制
1.6　深度學習的社會影響

深度學習的普及不僅是技術突破，也重新定義了「人工智慧」的邊界。
現今的 AI 模型能夠看（Computer Vision）、聽（Speech Recognition）、說（Natural Language Generation）、寫（Code Generation）、甚至理解跨模態關係（Multimodal Reasoning）。

這種「從數據中學習的能力」讓人工智慧從理論研究進入真實世界應用，包括：

醫學影像診斷（Radiology, Pathology）

自動駕駛（Autonomous Driving）

語音助手（Siri, Alexa）

機器翻譯（Google Translate）

生成式AI（ChatGPT, Stable Diffusion）

1.7　章節小結

人工智慧的演進歷程可視為「知識驅動 → 數據驅動 → 模型驅動」的三階段轉變。
深度學習正是第三階段的核心，它讓機器真正具備「自我學習」的能力，並在各領域開啟智慧革命。

1.8　思考與延伸練習

為什麼符號主義 AI 難以應對真實世界的不確定性？

試比較「人工特徵工程」與「自動特徵學習」的差異。

深度學習的崛起，與硬體發展（GPU/TPU）有何關聯？

若讓你設計一個能學會辨識音樂風格的模型，你會如何選擇架構與資料？

📘 延伸閱讀

Hinton, G. et al., “Deep Belief Nets,” Neural Computation, 2006.

LeCun, Y., Bengio, Y., & Hinton, G. (2015). “Deep Learning.” Nature, 521(7553), 436–444.

Goodfellow, I., Bengio, Y., Courville, A. (2016). Deep Learning. MIT Press.

第二章　神經網絡的數學基礎
2.1　導論：從生物神經元到數學模型

深度學習的核心在於模擬人腦如何接收、處理與傳遞訊息。人類神經系統由上百億個神經元（Neuron）組成，每個神經元會接收其他神經元的輸入訊號，進行運算後，再將結果透過軸突傳給下一個神經元。這個結構形成了龐大的神經網絡，使人腦能夠學習、記憶與判斷。

人工神經網絡（Artificial Neural Network, ANN）正是這一生物結構的數學抽象。
其基本單位是人工神經元（Artificial Neuron），用來模擬「輸入加權 → 累加 → 激活 → 輸出」的過程。

2.2　人工神經元模型：加權和與激活
2.2.1　數學定義

假設有輸入向量：

𝑥
=
[
𝑥
1
,
𝑥
2
,
.
.
.
,
𝑥
𝑛
]
𝑇
x=[x
1
	​

,x
2
	​

,...,x
n
	​

]
T

每一個輸入對應一個權重：

𝑤
=
[
𝑤
1
,
𝑤
2
,
.
.
.
,
𝑤
𝑛
]
𝑇
w=[w
1
	​

,w
2
	​

,...,w
n
	​

]
T

再加上一個偏置項 
𝑏
b，則神經元的輸出為：

𝑦
=
𝑓
(
𝑤
𝑇
𝑥
+
𝑏
)
y=f(w
T
x+b)

其中：

𝑓
(
⋅
)
f(⋅)：激活函數（activation function），引入非線性；

𝑤
𝑇
𝑥
w
T
x：輸入加權和；

𝑏
b：偏置項，控制輸出閾值。

這個過程可理解為：「將多個輸入訊號加權整合，經非線性轉換後，輸出一個數值」。

2.2.2　向量化表示

在深度學習實作中，矩陣運算能極大提升效率。
若同時有多個神經元組成一層，我們可以將整層表示為：

ℎ
=
𝑓
(
𝑊
𝑥
+
𝑏
)
h=f(Wx+b)

其中：

𝑊
W 為權重矩陣（形狀為 
𝑚
×
𝑛
m×n）；

𝑏
b 為偏置向量（長度為 
𝑚
m）；

𝑓
f 作用於每個元素（element-wise function）。

這是深度學習中最常見的運算結構，也是 GPU 平行化運算的基礎。

2.3　多層神經網絡（Multi-Layer Neural Network）
2.3.1　層的概念

一個典型的神經網絡由三個主要部分構成：

輸入層（Input Layer）：接收原始資料；

隱藏層（Hidden Layers）：進行非線性轉換與特徵提取；

輸出層（Output Layer）：產生最終結果。

如果我們將每一層的輸出再作為下一層的輸入，則整個網絡可以表示為：

𝑦
=
𝑓
𝐿
(
𝑊
𝐿
𝑓
𝐿
−
1
(
𝑊
𝐿
−
1
.
.
.
𝑓
1
(
𝑊
1
𝑥
+
𝑏
1
)
+
.
.
.
+
𝑏
𝐿
−
1
)
+
𝑏
𝐿
)
y=f
L
	​

(W
L
	​

f
L−1
	​

(W
L−1
	​

...f
1
	​

(W
1
	​

x+b
1
	​

)+...+b
L−1
	​

)+b
L
	​

)

這裡 
𝐿
L 代表網絡的層數。

2.3.2　函數組合觀點

神經網絡可以視為多個函數的組合：

𝐹
(
𝑥
)
=
𝑓
𝐿
∘
𝑓
𝐿
−
1
∘
.
.
.
∘
𝑓
1
(
𝑥
)
F(x)=f
L
	​

∘f
L−1
	​

∘...∘f
1
	​

(x)

也就是說，深度學習其實是學習「一個函數如何近似另一個未知函數」的過程。
這種觀點稱為函數逼近（Function Approximation）。

根據萬能逼近定理（Universal Approximation Theorem），只要隱藏層夠多、神經元足夠，神經網絡理論上可以逼近任意連續可微函數。
這是深度學習理論的數學根基。

2.4　前向傳播（Forward Propagation）

前向傳播是資料從輸入層經過隱藏層到輸出層的過程。

步驟：

將輸入資料 
𝑥
x 傳入第一層；

計算加權和 
𝑧
(
1
)
=
𝑊
(
1
)
𝑥
+
𝑏
(
1
)
z
(1)
=W
(1)
x+b
(1)
；

通過激活函數 
𝑎
(
1
)
=
𝑓
(
𝑧
(
1
)
)
a
(1)
=f(z
(1)
)；

重複以上步驟至輸出層；

得到最終預測結果 
𝑦
^
y
^
	​

。

整體可簡寫為：

𝑦
^
=
𝐹
(
𝑥
;
𝜃
)
y
^
	​

=F(x;θ)

其中 
𝜃
=
{
𝑊
,
𝑏
}
θ={W,b} 表示模型所有參數。

2.5　損失函數（Loss Function）

損失函數衡量模型預測值 
𝑦
^
y
^
	​

 與真實標籤 
𝑦
y 的差距。

2.5.1　常見損失函數
類型	損失函數	適用任務
均方誤差 (MSE)	
𝐿
=
1
𝑛
∑
𝑖
(
𝑦
𝑖
−
𝑦
^
𝑖
)
2
L=
n
1
	​

∑
i
	​

(y
i
	​

−
y
^
	​

i
	​

)
2
	迴歸問題
交叉熵 (Cross-Entropy)	
𝐿
=
−
∑
𝑖
𝑦
𝑖
log
⁡
(
𝑦
^
𝑖
)
L=−∑
i
	​

y
i
	​

log(
y
^
	​

i
	​

)	分類問題
Hinge Loss	
𝐿
=
max
⁡
(
0
,
1
−
𝑦
𝑖
𝑦
^
𝑖
)
L=max(0,1−y
i
	​

y
^
	​

i
	​

)	支援向量機 (SVM)
Kullback–Leibler Divergence	
𝐿
=
∑
𝑖
𝑝
𝑖
log
⁡
(
𝑝
𝑖
𝑞
𝑖
)
L=∑
i
	​

p
i
	​

log(
q
i
	​

p
i
	​

	​

)	分佈匹配

損失函數的選擇會直接影響模型的收斂速度與最終性能。

2.6　反向傳播（Backpropagation）

反向傳播是深度學習的靈魂。
其核心思想是利用鏈式法則（Chain Rule），計算每個參數對損失函數的偏導數，從而更新模型。

2.6.1　核心公式

假設輸出為：

𝑦
^
=
𝑓
(
𝑊
𝑥
+
𝑏
)
y
^
	​

=f(Wx+b)

損失函數為：

𝐿
=
1
2
(
𝑦
^
−
𝑦
)
2
L=
2
1
	​

(
y
^
	​

−y)
2

那麼每個權重的梯度為：

∂
𝐿
∂
𝑤
𝑖
=
(
𝑦
^
−
𝑦
)
𝑓
′
(
𝑧
)
𝑥
𝑖
∂w
i
	​

∂L
	​

=(
y
^
	​

−y)f
′
(z)x
i
	​


這樣每個權重都能根據誤差的方向進行修正。

2.6.2　梯度下降法（Gradient Descent）

模型更新規則：

𝜃
𝑡
+
1
=
𝜃
𝑡
−
𝜂
∂
𝐿
∂
𝜃
𝑡
θ
t+1
	​

=θ
t
	​

−η
∂θ
t
	​

∂L
	​


其中：

𝜂
η：學習率（Learning Rate）；

∂
𝐿
∂
𝜃
𝑡
∂θ
t
	​

∂L
	​

：當前梯度。

這樣模型會逐步沿著「損失函數下降最快的方向」逼近最小值。

2.6.3　變體：Mini-Batch 與 Adam

實務上不會一次用整個資料集更新參數，通常採用：

Batch Gradient Descent：整批更新；

Stochastic Gradient Descent (SGD)：每次一筆；

Mini-Batch Gradient Descent：折衷方案，效果最佳。

進階優化器如 Adam、RMSProp、Momentum 則在梯度方向加入動量或自適應學習率，加快收斂並避免震盪。

2.7　激活函數（Activation Function）

激活函數賦予模型非線性表達能力，使神經網絡能學習複雜關係。

函數	公式	範圍	優缺點
Sigmoid	
𝜎
(
𝑥
)
=
1
1
+
𝑒
−
𝑥
σ(x)=
1+e
−x
1
	​

	(0,1)	平滑但易梯度消失
Tanh	
tanh
⁡
(
𝑥
)
=
𝑒
𝑥
−
𝑒
−
𝑥
𝑒
𝑥
+
𝑒
−
𝑥
tanh(x)=
e
x
+e
−x
e
x
−e
−x
	​

	(-1,1)	平衡但仍飽和
ReLU	
𝑓
(
𝑥
)
=
max
⁡
(
0
,
𝑥
)
f(x)=max(0,x)	[0,∞)	高效但死區問題
Leaky ReLU	
𝑓
(
𝑥
)
=
max
⁡
(
0.01
𝑥
,
𝑥
)
f(x)=max(0.01x,x)	(-∞,∞)	緩解死區
GELU	
𝑓
(
𝑥
)
=
𝑥
Φ
(
𝑥
)
f(x)=xΦ(x)	(-∞,∞)	平滑連續，現代主流

ReLU 及其變體是現代神經網絡的事實標準，從 ResNet 到 Transformer 幾乎皆使用。

2.8　數學直觀：為何深度帶來力量？

從數學觀點，深度學習的「深」並非只是層數多，而是代表複合函數的層級表達能力。
每一層學習一種映射，並將結果輸入下一層，最終形成一個高度非線性的複雜函數。

這讓深度模型能夠：

擬合高度非線性關係；

自動分層學習抽象特徵；

以少量參數表達高維空間中的決策邊界。

這是傳統機器學習模型（如 SVM、KNN）無法比擬的。

2.9　章節小結
主題	重點摘要
神經元模型	加權求和 + 非線性激活
網絡結構	多層函數組合形成複雜映射
前向傳播	計算輸出
反向傳播	根據損失調整權重
損失函數	衡量預測與真值差距
激活函數	增加非線性表達力

神經網絡的數學本質，是函數學習（Function Learning）。
它透過梯度下降不斷修正參數，使模型逐步接近目標函數的真實形態。

2.10　延伸閱讀與練習

📘 閱讀建議

Bishop, C. M. Pattern Recognition and Machine Learning. Springer, 2006.

Goodfellow, I., Bengio, Y., Courville, A. Deep Learning. MIT Press, 2016.

Rumelhart, D. et al., “Learning Representations by Back-Propagating Errors,” Nature, 1986.

💡 思考題

為什麼梯度下降法會收斂？其幾何意義是什麼？

請比較 ReLU 與 Sigmoid 的優缺點，並說明在多層網絡中的梯度傳遞差異。

若學習率設定過大或過小，會發生什麼現象？

嘗試推導一個簡單的兩層神經網絡反向傳播過程。