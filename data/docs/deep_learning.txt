第一篇　深度學習的理論基礎
第一章　人工智慧與深度學習的演進
1.1　人工智慧的起源：從邏輯到學習

人工智慧（Artificial Intelligence, AI）的歷史可追溯至 1950 年代。當時的研究者試圖讓電腦模仿人類思考與推理。艾倫·圖靈（Alan Turing）於 1950 年提出著名的「圖靈測試（Turing Test）」，成為評估機器是否具備「智能」的重要概念依據。

**第一波 AI 浪潮（1950–1970）**以「符號主義（Symbolic AI）」為主。這種方法強調邏輯推理與規則系統，認為人類知識可以透過「邏輯命題」與「推論規則」加以表達。例如：

「如果 X 是鳥，那麼 X 會飛」

「如果企鵝是鳥，那麼企鵝不會飛」

這類知識可以編碼成專家系統（Expert System），如 MYCIN 與 DENDRAL。
然而，符號主義 AI 面臨兩個根本困難：

知識獲取瓶頸（Knowledge Acquisition Bottleneck）：需要人工輸入大量規則。

缺乏學習能力：當環境或資料變化時，系統無法自我調整。

1.2　第二波浪潮：機器學習的興起

1980–2000 年代，隨著資料量與運算能力提升，人工智慧進入第二波浪潮——機器學習（Machine Learning, ML）。此階段的核心思想是：「讓機器從資料中自動學習規則」，而非由人類手動設計。

傳統機器學習演算法（如決策樹、SVM、隨機森林、KNN）多依賴特徵工程（Feature Engineering）。也就是說，資料科學家需根據任務特性（如影像的顏色、紋理、形狀等）人工設計輸入特徵。

這種方法在中小型資料集上效果良好，但隨著資料維度（Dimension）爆炸式增加——特別是影像、語音與文字等**非結構化資料（Unstructured Data）**出現後，傳統方法逐漸顯得捉襟見肘。

1.3　第三波浪潮：深度學習的誕生

**深度學習（Deep Learning, DL）**源自於對「人工神經網絡（Artificial Neural Network, ANN）」的再發掘。

雖然神經網絡的概念早在 1957 年由 Rosenblatt 提出的「感知機（Perceptron）」就已存在，但由於當時的計算能力不足、理論基礎不穩，加上 Minsky 與 Papert 在 1969 年證明「單層感知機無法處理非線性問題（如 XOR）」後，神經網絡研究陷入低潮。

直到 2006 年，Hinton、LeCun 與 Bengio 等人提出「深層神經網絡（Deep Neural Network）」與「逐層預訓練（Layer-wise Pretraining）」技術，配合 GPU 平行運算與大數據資料集（如 ImageNet），深度學習才重新崛起，並掀起了第三次 AI 革命。

1.4　從人工設計到自我表徵

深度學習最核心的特徵是自動特徵學習（Automatic Feature Learning）。
在傳統 ML 中，特徵工程是由人類專家主導；但在 DL 中，模型本身會自動學習哪些特徵最能代表輸入資料。

以影像為例：

第一層學習邊緣（edges）；

第二層學習紋理（textures）；

第三層學習形狀（shapes）；

更高層學習物體結構（objects）。

這種逐層抽象的特徵表示方式稱為分層表徵（Hierarchical Representation），是深度學習能夠突破傳統限制的關鍵。

1.5　人工智慧的三大典範

AI 的三大主要學習範式可分為：

類型	核心概念	代表演算法	應用場景
監督式學習（Supervised Learning）	給模型「輸入 + 正確答案」學習	CNN、RNN、Transformer	圖像分類、語音辨識
非監督式學習（Unsupervised Learning）	只給輸入，模型自找規律	Autoencoder、GAN、Clustering	特徵壓縮、生成模型
強化學習（Reinforcement Learning）	模型在環境中試錯，根據獎勵調整策略	Q-Learning、PPO、DQN	AlphaGo、機器人控制
1.6　深度學習的社會影響

深度學習的普及不僅是技術突破，也重新定義了「人工智慧」的邊界。
現今的 AI 模型能夠看（Computer Vision）、聽（Speech Recognition）、說（Natural Language Generation）、寫（Code Generation）、甚至理解跨模態關係（Multimodal Reasoning）。

這種「從數據中學習的能力」讓人工智慧從理論研究進入真實世界應用，包括：

醫學影像診斷（Radiology, Pathology）

自動駕駛（Autonomous Driving）

語音助手（Siri, Alexa）

機器翻譯（Google Translate）

生成式AI（ChatGPT, Stable Diffusion）

1.7　章節小結

人工智慧的演進歷程可視為「知識驅動 → 數據驅動 → 模型驅動」的三階段轉變。
深度學習正是第三階段的核心，它讓機器真正具備「自我學習」的能力，並在各領域開啟智慧革命。

1.8　思考與延伸練習

為什麼符號主義 AI 難以應對真實世界的不確定性？

試比較「人工特徵工程」與「自動特徵學習」的差異。

深度學習的崛起，與硬體發展（GPU/TPU）有何關聯？

若讓你設計一個能學會辨識音樂風格的模型，你會如何選擇架構與資料？

📘 延伸閱讀

Hinton, G. et al., “Deep Belief Nets,” Neural Computation, 2006.

LeCun, Y., Bengio, Y., & Hinton, G. (2015). “Deep Learning.” Nature, 521(7553), 436–444.

Goodfellow, I., Bengio, Y., Courville, A. (2016). Deep Learning. MIT Press.

第二章　神經網絡的數學基礎
2.1　導論：從生物神經元到數學模型

深度學習的核心在於模擬人腦如何接收、處理與傳遞訊息。人類神經系統由上百億個神經元（Neuron）組成，每個神經元會接收其他神經元的輸入訊號，進行運算後，再將結果透過軸突傳給下一個神經元。這個結構形成了龐大的神經網絡，使人腦能夠學習、記憶與判斷。

人工神經網絡（Artificial Neural Network, ANN）正是這一生物結構的數學抽象。
其基本單位是人工神經元（Artificial Neuron），用來模擬「輸入加權 → 累加 → 激活 → 輸出」的過程。

2.2　人工神經元模型：加權和與激活
2.2.1　數學定義

假設有輸入向量：

𝑥
=
[
𝑥
1
,
𝑥
2
,
.
.
.
,
𝑥
𝑛
]
𝑇
x=[x
1
	​

,x
2
	​

,...,x
n
	​

]
T

每一個輸入對應一個權重：

𝑤
=
[
𝑤
1
,
𝑤
2
,
.
.
.
,
𝑤
𝑛
]
𝑇
w=[w
1
	​

,w
2
	​

,...,w
n
	​

]
T

再加上一個偏置項 
𝑏
b，則神經元的輸出為：

𝑦
=
𝑓
(
𝑤
𝑇
𝑥
+
𝑏
)
y=f(w
T
x+b)

其中：

𝑓
(
⋅
)
f(⋅)：激活函數（activation function），引入非線性；

𝑤
𝑇
𝑥
w
T
x：輸入加權和；

𝑏
b：偏置項，控制輸出閾值。

這個過程可理解為：「將多個輸入訊號加權整合，經非線性轉換後，輸出一個數值」。

2.2.2　向量化表示

在深度學習實作中，矩陣運算能極大提升效率。
若同時有多個神經元組成一層，我們可以將整層表示為：

ℎ
=
𝑓
(
𝑊
𝑥
+
𝑏
)
h=f(Wx+b)

其中：

𝑊
W 為權重矩陣（形狀為 
𝑚
×
𝑛
m×n）；

𝑏
b 為偏置向量（長度為 
𝑚
m）；

𝑓
f 作用於每個元素（element-wise function）。

這是深度學習中最常見的運算結構，也是 GPU 平行化運算的基礎。

2.3　多層神經網絡（Multi-Layer Neural Network）
2.3.1　層的概念

一個典型的神經網絡由三個主要部分構成：

輸入層（Input Layer）：接收原始資料；

隱藏層（Hidden Layers）：進行非線性轉換與特徵提取；

輸出層（Output Layer）：產生最終結果。

如果我們將每一層的輸出再作為下一層的輸入，則整個網絡可以表示為：

𝑦
=
𝑓
𝐿
(
𝑊
𝐿
𝑓
𝐿
−
1
(
𝑊
𝐿
−
1
.
.
.
𝑓
1
(
𝑊
1
𝑥
+
𝑏
1
)
+
.
.
.
+
𝑏
𝐿
−
1
)
+
𝑏
𝐿
)
y=f
L
	​

(W
L
	​

f
L−1
	​

(W
L−1
	​

...f
1
	​

(W
1
	​

x+b
1
	​

)+...+b
L−1
	​

)+b
L
	​

)

這裡 
𝐿
L 代表網絡的層數。

2.3.2　函數組合觀點

神經網絡可以視為多個函數的組合：

𝐹
(
𝑥
)
=
𝑓
𝐿
∘
𝑓
𝐿
−
1
∘
.
.
.
∘
𝑓
1
(
𝑥
)
F(x)=f
L
	​

∘f
L−1
	​

∘...∘f
1
	​

(x)

也就是說，深度學習其實是學習「一個函數如何近似另一個未知函數」的過程。
這種觀點稱為函數逼近（Function Approximation）。

根據萬能逼近定理（Universal Approximation Theorem），只要隱藏層夠多、神經元足夠，神經網絡理論上可以逼近任意連續可微函數。
這是深度學習理論的數學根基。

2.4　前向傳播（Forward Propagation）

前向傳播是資料從輸入層經過隱藏層到輸出層的過程。

步驟：

將輸入資料 
𝑥
x 傳入第一層；

計算加權和 
𝑧
(
1
)
=
𝑊
(
1
)
𝑥
+
𝑏
(
1
)
z
(1)
=W
(1)
x+b
(1)
；

通過激活函數 
𝑎
(
1
)
=
𝑓
(
𝑧
(
1
)
)
a
(1)
=f(z
(1)
)；

重複以上步驟至輸出層；

得到最終預測結果 
𝑦
^
y
^
	​

。

整體可簡寫為：

𝑦
^
=
𝐹
(
𝑥
;
𝜃
)
y
^
	​

=F(x;θ)

其中 
𝜃
=
{
𝑊
,
𝑏
}
θ={W,b} 表示模型所有參數。

2.5　損失函數（Loss Function）

損失函數衡量模型預測值 
𝑦
^
y
^
	​

 與真實標籤 
𝑦
y 的差距。

2.5.1　常見損失函數
類型	損失函數	適用任務
均方誤差 (MSE)	
𝐿
=
1
𝑛
∑
𝑖
(
𝑦
𝑖
−
𝑦
^
𝑖
)
2
L=
n
1
	​

∑
i
	​

(y
i
	​

−
y
^
	​

i
	​

)
2
	迴歸問題
交叉熵 (Cross-Entropy)	
𝐿
=
−
∑
𝑖
𝑦
𝑖
log
⁡
(
𝑦
^
𝑖
)
L=−∑
i
	​

y
i
	​

log(
y
^
	​

i
	​

)	分類問題
Hinge Loss	
𝐿
=
max
⁡
(
0
,
1
−
𝑦
𝑖
𝑦
^
𝑖
)
L=max(0,1−y
i
	​

y
^
	​

i
	​

)	支援向量機 (SVM)
Kullback–Leibler Divergence	
𝐿
=
∑
𝑖
𝑝
𝑖
log
⁡
(
𝑝
𝑖
𝑞
𝑖
)
L=∑
i
	​

p
i
	​

log(
q
i
	​

p
i
	​

	​

)	分佈匹配

損失函數的選擇會直接影響模型的收斂速度與最終性能。

2.6　反向傳播（Backpropagation）

反向傳播是深度學習的靈魂。
其核心思想是利用鏈式法則（Chain Rule），計算每個參數對損失函數的偏導數，從而更新模型。

2.6.1　核心公式

假設輸出為：

𝑦
^
=
𝑓
(
𝑊
𝑥
+
𝑏
)
y
^
	​

=f(Wx+b)

損失函數為：

𝐿
=
1
2
(
𝑦
^
−
𝑦
)
2
L=
2
1
	​

(
y
^
	​

−y)
2

那麼每個權重的梯度為：

∂
𝐿
∂
𝑤
𝑖
=
(
𝑦
^
−
𝑦
)
𝑓
′
(
𝑧
)
𝑥
𝑖
∂w
i
	​

∂L
	​

=(
y
^
	​

−y)f
′
(z)x
i
	​


這樣每個權重都能根據誤差的方向進行修正。

2.6.2　梯度下降法（Gradient Descent）

模型更新規則：

𝜃
𝑡
+
1
=
𝜃
𝑡
−
𝜂
∂
𝐿
∂
𝜃
𝑡
θ
t+1
	​

=θ
t
	​

−η
∂θ
t
	​

∂L
	​


其中：

𝜂
η：學習率（Learning Rate）；

∂
𝐿
∂
𝜃
𝑡
∂θ
t
	​

∂L
	​

：當前梯度。

這樣模型會逐步沿著「損失函數下降最快的方向」逼近最小值。

2.6.3　變體：Mini-Batch 與 Adam

實務上不會一次用整個資料集更新參數，通常採用：

Batch Gradient Descent：整批更新；

Stochastic Gradient Descent (SGD)：每次一筆；

Mini-Batch Gradient Descent：折衷方案，效果最佳。

進階優化器如 Adam、RMSProp、Momentum 則在梯度方向加入動量或自適應學習率，加快收斂並避免震盪。

2.7　激活函數（Activation Function）

激活函數賦予模型非線性表達能力，使神經網絡能學習複雜關係。

函數	公式	範圍	優缺點
Sigmoid	
𝜎
(
𝑥
)
=
1
1
+
𝑒
−
𝑥
σ(x)=
1+e
−x
1
	​

	(0,1)	平滑但易梯度消失
Tanh	
tanh
⁡
(
𝑥
)
=
𝑒
𝑥
−
𝑒
−
𝑥
𝑒
𝑥
+
𝑒
−
𝑥
tanh(x)=
e
x
+e
−x
e
x
−e
−x
	​

	(-1,1)	平衡但仍飽和
ReLU	
𝑓
(
𝑥
)
=
max
⁡
(
0
,
𝑥
)
f(x)=max(0,x)	[0,∞)	高效但死區問題
Leaky ReLU	
𝑓
(
𝑥
)
=
max
⁡
(
0.01
𝑥
,
𝑥
)
f(x)=max(0.01x,x)	(-∞,∞)	緩解死區
GELU	
𝑓
(
𝑥
)
=
𝑥
Φ
(
𝑥
)
f(x)=xΦ(x)	(-∞,∞)	平滑連續，現代主流

ReLU 及其變體是現代神經網絡的事實標準，從 ResNet 到 Transformer 幾乎皆使用。

2.8　數學直觀：為何深度帶來力量？

從數學觀點，深度學習的「深」並非只是層數多，而是代表複合函數的層級表達能力。
每一層學習一種映射，並將結果輸入下一層，最終形成一個高度非線性的複雜函數。

這讓深度模型能夠：

擬合高度非線性關係；

自動分層學習抽象特徵；

以少量參數表達高維空間中的決策邊界。

這是傳統機器學習模型（如 SVM、KNN）無法比擬的。

2.9　章節小結
主題	重點摘要
神經元模型	加權求和 + 非線性激活
網絡結構	多層函數組合形成複雜映射
前向傳播	計算輸出
反向傳播	根據損失調整權重
損失函數	衡量預測與真值差距
激活函數	增加非線性表達力

神經網絡的數學本質，是函數學習（Function Learning）。
它透過梯度下降不斷修正參數，使模型逐步接近目標函數的真實形態。

2.10　延伸閱讀與練習

📘 閱讀建議

Bishop, C. M. Pattern Recognition and Machine Learning. Springer, 2006.

Goodfellow, I., Bengio, Y., Courville, A. Deep Learning. MIT Press, 2016.

Rumelhart, D. et al., “Learning Representations by Back-Propagating Errors,” Nature, 1986.

💡 思考題

為什麼梯度下降法會收斂？其幾何意義是什麼？

請比較 ReLU 與 Sigmoid 的優缺點，並說明在多層網絡中的梯度傳遞差異。

若學習率設定過大或過小，會發生什麼現象？

嘗試推導一個簡單的兩層神經網絡反向傳播過程。

第三章　前向傳播與反向傳播
3.1　導論：從訊號流動到誤差修正

深度學習的核心精神，可以濃縮為兩個循環：

前向傳播（Forward Propagation）：將輸入資料送入網絡，計算預測結果。

反向傳播（Backpropagation）：根據預測誤差，計算每個參數的梯度並修正。

這兩個階段不斷交替進行，構成了深度學習的「學習循環（Learning Cycle）」。
換言之，前向是理解世界，反向是修正世界。

在數學上，前向傳播對應於「函數計算」，而反向傳播對應於「鏈式微分（chain rule differentiation）」。

3.2　前向傳播（Forward Propagation）詳解
3.2.1　概念流程

以一個三層神經網絡為例：

輸入層 → 隱藏層 → 輸出層
x → h → ŷ


每一層的運算都遵循以下規則：

𝑧
(
𝑙
)
=
𝑊
(
𝑙
)
𝑎
(
𝑙
−
1
)
+
𝑏
(
𝑙
)
z
(l)
=W
(l)
a
(l−1)
+b
(l)
𝑎
(
𝑙
)
=
𝑓
(
𝑙
)
(
𝑧
(
𝑙
)
)
a
(l)
=f
(l)
(z
(l)
)

其中：

𝑎
(
0
)
=
𝑥
a
(0)
=x 為輸入；

𝑊
(
𝑙
)
W
(l)
：第 
𝑙
l 層的權重矩陣；

𝑏
(
𝑙
)
b
(l)
：偏置向量；

𝑓
(
𝑙
)
(
⋅
)
f
(l)
(⋅)：激活函數。

最終，模型的輸出為：

𝑦
^
=
𝑎
(
𝐿
)
y
^
	​

=a
(L)
3.2.2　數學範例

假設我們要建立一個簡單的網絡：

輸入層：2 個節點；

隱藏層：2 個節點；

輸出層：1 個節點；

激活函數：Sigmoid。

則：

𝑧
(
1
)
	
=
𝑊
(
1
)
𝑥
+
𝑏
(
1
)


𝑎
(
1
)
	
=
𝜎
(
𝑧
(
1
)
)


𝑧
(
2
)
	
=
𝑊
(
2
)
𝑎
(
1
)
+
𝑏
(
2
)


𝑦
^
	
=
𝜎
(
𝑧
(
2
)
)
z
(1)
a
(1)
z
(2)
y
^
	​

	​

=W
(1)
x+b
(1)
=σ(z
(1)
)
=W
(2)
a
(1)
+b
(2)
=σ(z
(2)
)
	​


整個過程類似電流流過電路，每層的輸出成為下一層的輸入。
前向傳播只需矩陣乘法與非線性轉換，因此能在 GPU 上高度平行化。

3.2.3　範例：手寫數字辨識（MNIST）

以 MNIST 手寫數字（28×28 灰階影像）為例，輸入層為 784 個節點。
假設我們構建如下網絡：

層級	神經元數量	激活函數
輸入層	784	-
隱藏層1	128	ReLU
隱藏層2	64	ReLU
輸出層	10	Softmax

在前向傳播時，每一層將輸入映射至高維空間並抽取特徵，最終輸出 10 維機率向量，對應每個數字的分類概率。

3.3　損失函數（Loss Function）在前向階段的角色

損失函數是模型「學習方向的指南針」。
在前向階段，我們計算：

𝐿
=
𝐿
(
𝑦
^
,
𝑦
)
L=L(
y
^
	​

,y)

常見範例：

(1) 均方誤差（Mean Squared Error, MSE）
𝐿
=
1
2
𝑛
∑
𝑖
=
1
𝑛
(
𝑦
𝑖
−
𝑦
^
𝑖
)
2
L=
2n
1
	​

i=1
∑
n
	​

(y
i
	​

−
y
^
	​

i
	​

)
2
(2) 交叉熵（Cross Entropy）
𝐿
=
−
∑
𝑖
=
1
𝐶
𝑦
𝑖
log
⁡
(
𝑦
^
𝑖
)
L=−
i=1
∑
C
	​

y
i
	​

log(
y
^
	​

i
	​

)

此式特別適合分類任務，其中 
𝐶
C 為類別數。

3.4　反向傳播（Backpropagation）原理

反向傳播是深度學習的靈魂，它使得神經網絡能夠「從錯誤中學習」。

其核心思想是：

當輸出誤差被計算出來後，利用微分將這個誤差「沿著網絡結構反向傳播」至每個權重，並根據貢獻度進行調整。

3.4.1　鏈式法則（Chain Rule）

如果 
𝑦
=
𝑓
(
𝑢
)
y=f(u)，而 
𝑢
=
𝑔
(
𝑥
)
u=g(x)，那麼：

𝑑
𝑦
𝑑
𝑥
=
𝑑
𝑦
𝑑
𝑢
⋅
𝑑
𝑢
𝑑
𝑥
dx
dy
	​

=
du
dy
	​

⋅
dx
du
	​


神經網絡正是多層函數的組合，因此可遞迴地應用鏈式法則。
例如對於第 
𝑙
l 層的權重 
𝑊
(
𝑙
)
W
(l)
，其梯度為：

∂
𝐿
∂
𝑊
(
𝑙
)
=
∂
𝐿
∂
𝑎
(
𝑙
)
⋅
∂
𝑎
(
𝑙
)
∂
𝑧
(
𝑙
)
⋅
∂
𝑧
(
𝑙
)
∂
𝑊
(
𝑙
)
∂W
(l)
∂L
	​

=
∂a
(l)
∂L
	​

⋅
∂z
(l)
∂a
(l)
	​

⋅
∂W
(l)
∂z
(l)
	​

3.4.2　誤差項（Error Term）定義

為了簡化計算，引入「誤差項」符號：

𝛿
(
𝑙
)
=
∂
𝐿
∂
𝑧
(
𝑙
)
δ
(l)
=
∂z
(l)
∂L
	​


則：

𝛿
(
𝑙
)
=
(
(
𝑊
(
𝑙
+
1
)
)
𝑇
𝛿
(
𝑙
+
1
)
)
⊙
𝑓
′
(
𝑧
(
𝑙
)
)
δ
(l)
=((W
(l+1)
)
T
δ
(l+1)
)⊙f
′
(z
(l)
)

這個公式揭示了反向傳播的遞迴本質：
每一層的誤差，取決於下一層的誤差以及該層激活函數的導數。

3.4.3　梯度更新

權重與偏置的更新規則為：

𝑊
(
𝑙
)
←
𝑊
(
𝑙
)
−
𝜂
∂
𝐿
∂
𝑊
(
𝑙
)
W
(l)
←W
(l)
−η
∂W
(l)
∂L
	​

𝑏
(
𝑙
)
←
𝑏
(
𝑙
)
−
𝜂
∂
𝐿
∂
𝑏
(
𝑙
)
b
(l)
←b
(l)
−η
∂b
(l)
∂L
	​


其中 
𝜂
η 為學習率。

這樣的更新使得模型在每一次訓練迭代（epoch）中逐步修正預測誤差。

3.5　完整演算法流程
【Algorithm 1】前向與反向傳播訓練流程

1️⃣ 初始化參數：隨機生成 
𝑊
(
𝑙
)
,
𝑏
(
𝑙
)
W
(l)
,b
(l)

2️⃣ 輸入資料：將訓練樣本 
𝑥
x 傳入網絡
3️⃣ 前向傳播：

𝑎
(
𝑙
)
=
𝑓
(
𝑊
(
𝑙
)
𝑎
(
𝑙
−
1
)
+
𝑏
(
𝑙
)
)
a
(l)
=f(W
(l)
a
(l−1)
+b
(l)
)

直到輸出層得到 
𝑦
^
y
^
	​


4️⃣ 計算損失：

𝐿
=
𝐿
(
𝑦
^
,
𝑦
)
L=L(
y
^
	​

,y)

5️⃣ 反向傳播：
從輸出層開始遞迴計算：

𝛿
(
𝐿
)
=
∇
𝑦
^
𝐿
⊙
𝑓
′
(
𝑧
(
𝐿
)
)
δ
(L)
=∇
y
^
	​

	​

L⊙f
′
(z
(L)
)
𝛿
(
𝑙
)
=
(
(
𝑊
(
𝑙
+
1
)
)
𝑇
𝛿
(
𝑙
+
1
)
)
⊙
𝑓
′
(
𝑧
(
𝑙
)
)
δ
(l)
=((W
(l+1)
)
T
δ
(l+1)
)⊙f
′
(z
(l)
)

6️⃣ 更新權重：

𝑊
(
𝑙
)
←
𝑊
(
𝑙
)
−
𝜂
𝛿
(
𝑙
)
(
𝑎
(
𝑙
−
1
)
)
𝑇
W
(l)
←W
(l)
−ηδ
(l)
(a
(l−1)
)
T

7️⃣ 重複步驟 2–6，直到收斂。

3.6　視覺化直覺

從圖形角度理解：

Forward : 資料流動 →
Backward : 誤差流動 ←


每一層在前向階段「生成」表徵，在反向階段「修正」表徵。
這種「雙向能量流」的結構，使深度學習能持續自我調整，如同人類學習過程中「感知 → 錯誤 → 修正」的循環。

3.7　實例推導：兩層網絡的反向傳播

考慮一個兩層網絡：

𝑦
^
=
𝑓
2
(
𝑊
2
𝑓
1
(
𝑊
1
𝑥
+
𝑏
1
)
+
𝑏
2
)
y
^
	​

=f
2
	​

(W
2
	​

f
1
	​

(W
1
	​

x+b
1
	​

)+b
2
	​

)

損失函數為：

𝐿
=
1
2
(
𝑦
−
𝑦
^
)
2
L=
2
1
	​

(y−
y
^
	​

)
2

反向傳播步驟：

1️⃣ 計算輸出誤差：

𝛿
(
2
)
=
(
𝑦
^
−
𝑦
)
𝑓
2
′
(
𝑧
(
2
)
)
δ
(2)
=(
y
^
	​

−y)f
2
′
	​

(z
(2)
)

2️⃣ 傳回隱藏層：

𝛿
(
1
)
=
(
𝑊
2
𝑇
𝛿
(
2
)
)
𝑓
1
′
(
𝑧
(
1
)
)
δ
(1)
=(W
2
T
	​

δ
(2)
)f
1
′
	​

(z
(1)
)

3️⃣ 計算梯度：

∂
𝐿
∂
𝑊
2
=
𝛿
(
2
)
(
𝑎
(
1
)
)
𝑇
∂W
2
	​

∂L
	​

=δ
(2)
(a
(1)
)
T
∂
𝐿
∂
𝑊
1
=
𝛿
(
1
)
(
𝑥
)
𝑇
∂W
1
	​

∂L
	​

=δ
(1)
(x)
T

4️⃣ 更新：

𝑊
𝑖
←
𝑊
𝑖
−
𝜂
∂
𝐿
∂
𝑊
𝑖
W
i
	​

←W
i
	​

−η
∂W
i
	​

∂L
	​


這是最基本的反向傳播形式，幾乎所有深度架構（CNN、RNN、Transformer）皆在此基礎上延伸。

3.8　數值穩定性與梯度問題

深層網絡容易出現：

梯度消失（Vanishing Gradient）：梯度太小導致權重幾乎不更新；

梯度爆炸（Exploding Gradient）：梯度太大導致更新不穩定。

這些問題的成因在於：鏈式法則中的乘積連乘導致指數放大或縮小。

解法：

使用 ReLU 類激活函數（避免飽和區間）；

權重初始化策略（如 Xavier 或 He 初始化）；

正規化層（Batch Normalization）；

梯度裁剪（Gradient Clipping）；

使用 Adam、RMSProp 等穩定優化器。

3.9　前向與反向的數學對稱性

值得注意的是：

前向傳播 = 「矩陣乘法 + 非線性轉換」；

反向傳播 = 「矩陣轉置 + 鏈式法則」。

這種數學對稱性是深度學習高效運算的核心。
在實作上，框架如 PyTorch、TensorFlow 都能自動進行反向微分（Automatic Differentiation, Autograd），避免手動推導錯誤。

3.10　章節小結
概念	說明
前向傳播	計算模型輸出，產生預測結果
損失函數	評估模型誤差
反向傳播	透過鏈式法則計算梯度
梯度下降	沿最速下降方向更新參數
自動微分	現代框架的核心技術
梯度問題	使用 ReLU、BatchNorm、Adam 等解法

深度學習之所以強大，不在於單一數學技巧，而在於它結合了數學的可導性、電腦的平行化與資料的冗餘性。
前向與反向的反覆交替，就像「呼吸」一樣，使模型在龐大的參數空間中逐步逼近真實世界的分佈。

3.11　延伸閱讀與練習

📗 閱讀建議

Rumelhart, D. et al., “Learning Representations by Back-Propagating Errors,” Nature, 1986.

LeCun, Y. et al., “Efficient BackProp,” Neural Networks: Tricks of the Trade, Springer, 2012.

Goodfellow, I. et al., Deep Learning, MIT Press, Chapter 6–8.

💡 練習題

推導三層神經網絡的完整反向傳播公式。

解釋梯度消失問題與激活函數之關聯。

實作一個兩層 ReLU 網絡，手動驗證前向與反向結果是否一致。

思考：如果反向傳播中學習率為負，會發生什麼？