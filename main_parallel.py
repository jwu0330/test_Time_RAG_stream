"""
ä¸»ç¨‹åº - é›™ç·šç¨‹ç‰ˆæœ¬
å¯¦ç¾ä¸»ç·šï¼ˆRAGæ•™æç”Ÿæˆï¼‰å’Œåˆ†æ”¯ï¼ˆæƒ…å¢ƒåˆ¤å®šï¼‰ä¸¦è¡Œè™•ç†
"""
import asyncio
import os
from datetime import datetime
from typing import Dict, List, Optional
from pathlib import Path
from openai import OpenAI

from core.vector_store import VectorStore
from core.rag_module import RAGRetriever, RAGCache
from core.scenario_classifier import ScenarioClassifier
from core.ontology_manager import OntologyManager
from core.history_manager import HistoryManager
from core.timer_utils import Timer
from config import Config


class ParallelRAGSystem:
    """é›™ç·šç¨‹ä¸¦è¡Œè™•ç†çš„ RAG ç³»çµ±"""
    
    def __init__(self, api_key: Optional[str] = None):
        """
        åˆå§‹åŒ–ç³»çµ±
        
        Args:
            api_key: OpenAI API Key
        """
        self.api_key = api_key
        self.client = OpenAI(api_key=api_key) if api_key else OpenAI()
        
        # åˆå§‹åŒ–å„æ¨¡çµ„
        self.vector_store = VectorStore(api_key=api_key)
        self.rag_retriever = RAGRetriever(self.vector_store)
        self.rag_cache = RAGCache()
        self.scenario_classifier = ScenarioClassifier(api_key=api_key)
        self.ontology_manager = OntologyManager()
        self.history_manager = HistoryManager()
        
        # è¨ˆæ™‚å™¨
        self.timer = Timer()
        
        print("ğŸš€ é›™ç·šç¨‹ RAG ç³»çµ±å·²åˆå§‹åŒ–")
    
    async def initialize_documents(self, docs_dir: str = None):
        """
        åˆå§‹åŒ–æ–‡ä»¶å‘é‡åŒ–
        
        Args:
            docs_dir: æ–‡ä»¶ç›®éŒ„
        """
        docs_dir = docs_dir or Config.DOCS_DIR
        
        print(f"\nğŸ“š åˆå§‹åŒ–æ–‡ä»¶å‘é‡...")
        self.timer.start_stage("å‘é‡åŒ–")
        
        # æª¢æŸ¥æ˜¯å¦å·²æœ‰å‘é‡æ–‡ä»¶
        if self.vector_store.load():
            self.timer.stop_stage("å‘é‡åŒ–")
            print("âœ… ä½¿ç”¨å·²å„²å­˜çš„å‘é‡ï¼ˆå¿«é€Ÿå•Ÿå‹•ï¼‰")
            return
        
        # ç¬¬ä¸€æ¬¡å•Ÿå‹•ï¼Œéœ€è¦å‘é‡åŒ–
        print("âš ï¸  é¦–æ¬¡å•Ÿå‹•ï¼Œéœ€è¦èª¿ç”¨ OpenAI API ç”Ÿæˆå‘é‡")
        print("â³ é è¨ˆéœ€è¦ 10-15 ç§’ï¼Œè«‹ç¨å€™...")
        
        # è®€å–ä¸¦å‘é‡åŒ–æ–‡ä»¶
        if not os.path.exists(docs_dir):
            print(f"âš ï¸  æ–‡ä»¶ç›®éŒ„ä¸å­˜åœ¨: {docs_dir}")
            self.timer.stop_stage("å‘é‡åŒ–")
            return
        
        documents = []
        for filename in os.listdir(docs_dir):
            filepath = os.path.join(docs_dir, filename)
            if os.path.isfile(filepath) and filename.endswith('.txt'):
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                    documents.append({
                        "id": filename,
                        "content": content,
                        "metadata": {"filename": filename}
                    })
                    print(f"  ğŸ“„ è¼‰å…¥: {filename} ({len(content)} å­—)")
        
        if documents:
            await self.vector_store.batch_add_documents(documents)
            self.vector_store.save()
            self.vector_store.export_to_json()
        
        self.timer.stop_stage("å‘é‡åŒ–")
    
    async def main_thread_rag(self, query: str) -> Dict:
        """
        ä¸»ç·šï¼šRAG æª¢ç´¢ + æ•™æç”Ÿæˆ
        
        Args:
            query: ç”¨æˆ¶æŸ¥è©¢
            
        Returns:
            RAG æª¢ç´¢çµæœå’Œè‰ç¨¿ç­”æ¡ˆ
        """
        print("ã€ä¸»ç·šã€‘é–‹å§‹ RAG æª¢ç´¢...")
        
        # RAG æª¢ç´¢
        retrieved_docs = await self.rag_retriever.retrieve(query, top_k=3)
        context = self.rag_retriever.format_context(retrieved_docs)
        matched_doc_ids = self.rag_retriever.get_matched_doc_ids(retrieved_docs)
        
        # æå–çŸ¥è­˜é»
        knowledge_points = []
        for doc_id in matched_doc_ids:
            if doc_id in Config.KNOWLEDGE_POINTS:
                knowledge_points.append(Config.KNOWLEDGE_POINTS[doc_id])
        
        # ç”Ÿæˆè‰ç¨¿ç­”æ¡ˆ
        draft_prompt = f"""
æ ¹æ“šä»¥ä¸‹æ•™æå…§å®¹å›ç­”å•é¡Œã€‚

ã€æ•™æå…§å®¹ã€‘
{context}

ã€å•é¡Œã€‘
{query}

è«‹æä¾›åˆæ­¥ç­”æ¡ˆï¼š
"""
        
        response = self.client.chat.completions.create(
            model=Config.LLM_MODEL,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„çŸ¥è­˜åŠ©æ‰‹ã€‚"},
                {"role": "user", "content": draft_prompt}
            ],
            temperature=Config.LLM_TEMPERATURE,
            max_tokens=Config.LLM_MAX_TOKENS
        )
        
        draft_answer = response.choices[0].message.content
        
        print("ã€ä¸»ç·šã€‘RAG æª¢ç´¢å®Œæˆ")
        
        return {
            "draft_answer": draft_answer,
            "context": context,
            "matched_docs": matched_doc_ids,
            "knowledge_points": knowledge_points,
            "retrieved_docs": retrieved_docs
        }
    
    async def branch_thread_scenario(self, query: str) -> Dict:
        """
        åˆ†æ”¯ï¼šæƒ…å¢ƒåˆ¤å®š
        
        Args:
            query: ç”¨æˆ¶æŸ¥è©¢
            
        Returns:
            æƒ…å¢ƒåˆ¤å®šçµæœ
        """
        print("ã€åˆ†æ”¯ã€‘é–‹å§‹æƒ…å¢ƒåˆ¤å®š...")
        
        # ç²å–æ­·å²è¨˜éŒ„
        history = self.history_manager.get_recent_history(n=5)
        
        # å‘¼å« API é€²è¡Œå››å‘åº¦åˆ¤å®š
        result = self.scenario_classifier.classify(query, history=history)
        
        print("ã€åˆ†æ”¯ã€‘æƒ…å¢ƒåˆ¤å®šå®Œæˆ")
        
        return result
    
    async def merge_and_generate(
        self, 
        rag_result: Dict, 
        scenario_result: Dict, 
        query: str
    ) -> str:
        """
        æœƒè¨ºï¼šåˆä½µå…©æ¢ç·šçš„çµæœä¸¦ç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ
        
        Args:
            rag_result: ä¸»ç·šçš„ RAG çµæœ
            scenario_result: åˆ†æ”¯çš„æƒ…å¢ƒåˆ¤å®šçµæœ
            query: ç”¨æˆ¶å•é¡Œ
            
        Returns:
            æœ€çµ‚ç­”æ¡ˆ
        """
        print("\nã€æœƒè¨ºã€‘åˆä½µå…©æ¢ç·šçš„çµæœ...")
        
        # æå–çµæœ
        draft_answer = rag_result['draft_answer']
        context = rag_result['context']
        knowledge_points = rag_result['knowledge_points']
        
        scenario_number = scenario_result['scenario_number']
        dimensions = scenario_result['dimensions']
        
        # æ§‹å»ºæƒ…å¢ƒèªªæ˜æ–‡å­—
        scenario_text = f"ç¾åœ¨ç‚ºç¬¬ {scenario_number} ç¨®æƒ…å¢ƒï¼Œåˆ†åˆ¥ä»£è¡¨ D1={dimensions['D1']}, D2={dimensions['D2']}, D3={dimensions['D3']}, D4={dimensions['D4']}"
        
        # è¼‰å…¥æœ¬é«”è«–ï¼ˆä½œç‚ºæ•™æçš„ä¸€éƒ¨åˆ†ï¼‰
        ontology_content = self.ontology_manager.get_ontology_content()
        
        # æ§‹å»ºæœ€çµ‚æç¤ºè©
        final_prompt = f"""
ã€åˆæ­¥ç­”æ¡ˆã€‘
{draft_answer}

ã€æƒ…å¢ƒè³‡è¨Šã€‘
{scenario_text}

ã€çŸ¥è­˜æœ¬é«”è«–ã€‘
{ontology_content}

ã€å•é¡Œã€‘
{query}

è«‹æ ¹æ“šä»¥ä¸Šæƒ…å¢ƒè³‡è¨Šå’Œæ•™æå…§å®¹ï¼Œèª¿æ•´ä¸¦ç”Ÿæˆæœ€çµ‚å›ç­”ã€‚åœ¨å›ç­”ä¸­åŠ å…¥ï¼šã€Œ{scenario_text}ã€
"""
        
        print(f"ã€æœƒè¨ºã€‘æƒ…å¢ƒè³‡è¨Šï¼š{scenario_text}")
        
        # ç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ
        response = self.client.chat.completions.create(
            model=Config.LLM_MODEL,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„çŸ¥è­˜åŠ©æ‰‹ã€‚"},
                {"role": "user", "content": final_prompt}
            ],
            temperature=Config.LLM_TEMPERATURE,
            max_tokens=Config.LLM_FINAL_MAX_TOKENS,
            stream=True
        )
        
        print("\nğŸ’¬ ç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ...")
        print("-" * 60)
        
        final_answer = ""
        for chunk in response:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                print(content, end="", flush=True)
                final_answer += content
        
        print("\n" + "-" * 60)
        
        return final_answer
    
    async def process_query(self, query: str) -> Dict:
        """
        è™•ç†æŸ¥è©¢ï¼ˆé›™ç·šç¨‹ç‰ˆæœ¬ï¼‰
        
        Args:
            query: ç”¨æˆ¶æŸ¥è©¢
            
        Returns:
            å®Œæ•´çµæœ
        """
        print("\n" + "="*70)
        print(f"ğŸ” è™•ç†æŸ¥è©¢: {query}")
        print("="*70)
        
        # é‡ç½®è¨ˆæ™‚å™¨
        self.timer = Timer()
        self.timer.start_stage("ç¸½æµç¨‹")
        
        # ============ é›™ç·šç¨‹ä¸¦è¡Œè™•ç† ============
        print("\nğŸš€ å•Ÿå‹•é›™ç·šç¨‹ä¸¦è¡Œè™•ç†...\n")
        self.timer.start_stage("ä¸¦è¡Œè™•ç†")
        
        # åŒæ™‚å•Ÿå‹•å…©æ¢ç·š
        main_task = self.main_thread_rag(query)
        branch_task = self.branch_thread_scenario(query)
        
        # ç­‰å¾…å…©æ¢ç·šéƒ½å®Œæˆ
        rag_result, scenario_result = await asyncio.gather(main_task, branch_task)
        
        self.timer.stop_stage("ä¸¦è¡Œè™•ç†")
        print("\nâœ… å…©æ¢ç·šéƒ½å·²å®Œæˆ\n")
        
        # ============ æœƒè¨ºï¼šåˆä½µçµæœ ============
        self.timer.start_stage("æœƒè¨ºç”Ÿæˆ")
        
        final_answer = await self.merge_and_generate(rag_result, scenario_result, query)
        
        self.timer.stop_stage("æœƒè¨ºç”Ÿæˆ")
        self.timer.stop_stage("ç¸½æµç¨‹")
        
        # è¨˜éŒ„åˆ°æ­·å²
        dimensions_dict = {
            "D1": "ä¸€å€‹",  # TODO: å¾ RAG çµæœè¨ˆç®—
            "D2": scenario_result['dimensions']['D2'],
            "D3": scenario_result['dimensions']['D3'],
            "D4": scenario_result['dimensions']['D4']
        }
        
        self.history_manager.add_query(
            query,
            rag_result['matched_docs'],
            dimensions_dict
        )
        
        # ============ è¿”å›çµæœ ============
        result = {
            "query": query,
            "final_answer": final_answer,
            "scenario_number": scenario_result['scenario_number'],
            "scenario_description": scenario_result['description'],
            "dimensions": scenario_result['dimensions'],
            "matched_docs": rag_result['matched_docs'],
            "knowledge_points": rag_result['knowledge_points'],
            "time_report": self.timer.get_report().to_dict()
        }
        
        return result
    
    def print_summary(self, result: Dict):
        """æ‰“å°çµæœæ‘˜è¦"""
        print("\n" + "="*70)
        print("ğŸ“Š è™•ç†çµæœæ‘˜è¦")
        print("="*70)
        print(f"æŸ¥è©¢ï¼š{result['query']}")
        print(f"\næƒ…å¢ƒï¼šç¬¬ {result['scenario_number']} ç¨®")
        print(f"æè¿°ï¼š{result['scenario_description']}")
        print(f"\nå››å‘åº¦ï¼š")
        for dim, value in result['dimensions'].items():
            print(f"  {dim}: {value}")
        print(f"\nçŸ¥è­˜é»ï¼š{', '.join(result['knowledge_points']) if result['knowledge_points'] else 'ç„¡'}")
        print(f"\næ™‚é–“å ±å‘Šï¼š")
        for stage, duration in result['time_report']['stages'].items():
            print(f"  {stage}: {duration:.3f}s")
        print(f"  ç¸½è¨ˆ: {result['time_report']['total_time']:.3f}s")
        print("="*70)


async def main():
    """ä¸»å‡½æ•¸"""
    print("\n" + "="*70)
    print("ğŸš€ é›™ç·šç¨‹ RAG ç³»çµ±")
    print("="*70)
    
    # åˆå§‹åŒ–ç³»çµ±
    system = ParallelRAGSystem()
    
    # åˆå§‹åŒ–æ–‡ä»¶
    await system.initialize_documents()
    
    # æ¸¬è©¦æŸ¥è©¢
    test_queries = [
        "ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Ÿ",
        "æ©Ÿå™¨å­¸ç¿’å’Œæ·±åº¦å­¸ç¿’æœ‰ä»€éº¼å€åˆ¥ï¼Ÿ"
    ]
    
    for query in test_queries:
        result = await system.process_query(query)
        system.print_summary(result)
        print("\n")


if __name__ == "__main__":
    asyncio.run(main())
